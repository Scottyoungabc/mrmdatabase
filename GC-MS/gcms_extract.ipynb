{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Please ensure that this script is in the same directory as the text file (Mona_experimental_ms.txt)\n",
    "Run the following code block in sequence, some code block might take up to half an hour (depending on computer speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm  #instant showing the progress of loops \n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\n",
    "    'Name', \n",
    "    'Formula',\n",
    "    'MW',\n",
    "    'Exact Mass',\n",
    "    'NIST#',\n",
    "    'InChIKey', \n",
    "    'RI_estimated'\n",
    "    'RI_predicted'\n",
    "    'MS'\n",
    "    'Score'\n",
    "    #ms intensity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_filename = 'GC-MSMS-NIST20_2.txt' # name of the raw data text file\n",
    "foldername = 'GCMS_experiments' # this is the folder that we are going to store our cleaned data\n",
    "ignore = False # variable used to determine whether to exclude the element\n",
    "file_exist = True # variable used to determine whether the file that we are going to append exist or not\n",
    "repeating_key = False # variable used to determine whether there are multiple fields with same name in the element\n",
    "item_dict = {} # dictionary to store all the fields and value for an element\n",
    "string = '' # element in string form that are going to be written into error.txt when we can't decide its filename\n",
    "filename = '' # name of the file to store the cleaned data\n",
    "inchikey, mw= '','' # these variables are used to determine whether to include or exclude the element\n",
    "\n",
    "if not (os.path.exists(foldername)):\n",
    "    os.makedirs(foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code block might take around 8 minutes to run\n",
    "\n",
    "with open(txt_filename, mode='r', encoding='utf-8-sig') as readf:\n",
    "    while True:\n",
    "        row = readf.readline()\n",
    "        \n",
    "        #use name as the collection starts\n",
    "        if (row == 'Name'):\n",
    "            readf.readline()\n",
    "            \n",
    "            # We ignore the current item if InChIKey is missing\n",
    "            if not(inchikey):\n",
    "                ignore = True\n",
    "            \n",
    "            if not (ignore):\n",
    "                # if there are multiple fields with same name in the current element,\n",
    "                # we output them into repeating_key.csv for further debugging purpose\n",
    "                if (repeating_key):\n",
    "                    filename = os.path.join(os.getcwd(), 'repeating_key.csv')\n",
    "\n",
    "                if (filename):\n",
    "                        \n",
    "                    file_exist = os.path.isfile(filename)\n",
    "\n",
    "                    with open(filename, 'a', encoding='utf-8-sig', newline='') as writef:\n",
    "                        writer = csv.writer(writef)\n",
    "                        if not (file_exist):\n",
    "                            writer.writerow(header)\n",
    "                        \n",
    "                        ms = item_dict['MS']\n",
    "                        score = item_dict['Score']\n",
    "                        while (len(ms) > 0):\n",
    "                            item_dict['MS'] = ms.pop(0)\n",
    "                            item_dict['Score'] = score.pop(0)\n",
    "                            writer.writerow(item_dict.get(col, '') for col in header)\n",
    "\n",
    "                else:\n",
    "                    # Will generate error.txt when:\n",
    "                    # 1. PrecursorMZ and MW are both not found\n",
    "                    # 2. There is trailing space or newline at the end of the file\n",
    "                    with open('error.txt', 'a', encoding='utf-8-sig') as writef:\n",
    "                        writef.write(string)\n",
    "                        writef.write('\\n')\n",
    "                    \n",
    "            # reinitialize everything after writing an item into a file\n",
    "            ignore = False\n",
    "            file_exist = True\n",
    "            repeating_key = False\n",
    "            item_dict = {}\n",
    "            string = ''\n",
    "            filename = ''\n",
    "            inchikey= ''\n",
    "            \n",
    "            if (row == 'Name'):\n",
    "                break # break out of the loop when we reach EOF\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # Ignore item with precursor_type != '[M+H]+'\n",
    "#         if ('Precursor_type: ' in row):\n",
    "#             precursor_type = row.strip('\\n').split(\": \")[1]\n",
    "#             if not (precursor_type == '[M+H]+'):\n",
    "                \n",
    "#                 # reinitialize everything after we ignore the current element\n",
    "#                 ignore = False\n",
    "#                 file_exist = True\n",
    "#                 repeating_key = False\n",
    "#                 item_dict = {}\n",
    "#                 string = ''\n",
    "#                 filename = ''\n",
    "#                 inchikey, spectrum_type, precursor_type, precursor_mz, mw = '', '', '', '', ''\n",
    "                \n",
    "#                 # read the rest of the line for that element until the next element\n",
    "#                 row = readf.readline()\n",
    "#                 while not (row == '\\n' or row == ''):\n",
    "#                     row = readf.readline()\n",
    "#                 else:\n",
    "#                     readf.readline()\n",
    "#                     continue\n",
    "        \n",
    "#         # Ignore item with spectrum_type != 'MS2'\n",
    "#         if ('Spectrum_type: ' in row):\n",
    "#             spectrum_type = row.strip('\\n').split(\": \")[1]\n",
    "#             if not (spectrum_type == 'MS2'):\n",
    "                \n",
    "#                 # reinitialize everything after we ignore the current element\n",
    "#                 ignore = False\n",
    "#                 file_exist = True\n",
    "#                 repeating_key = False\n",
    "#                 item_dict = {}\n",
    "#                 string = ''\n",
    "#                 filename = ''\n",
    "#                 inchikey, spectrum_type, precursor_type, precursor_mz, mw = '', '', '', '', ''\n",
    "                \n",
    "#                 # read the rest of the line for that element until the next element\n",
    "#                 row = readf.readline()\n",
    "#                 while not (row == '\\n' or row == ''):\n",
    "#                     row = readf.readline()\n",
    "#                 else:\n",
    "#                     readf.readline()\n",
    "#                     continue\n",
    "                \n",
    "        # Store InChIKey if it exists, used for InChIKey existence check later on\n",
    "        if ('InChIKey: ' in row):\n",
    "            inchikey = row.strip('\\n').split(\": \")[1]\n",
    "        \n",
    "        # exclude these fields\n",
    "#         if ('Synon: ' in row or 'Comments: ' in row or 'Num Peaks: ' in row):\n",
    "        if ('Synonyms: ' in row or 'Values and Intensities: ' in row):\n",
    "            continue\n",
    "            \n",
    "        # determine the filename by using either PrecursorMZ or MW+1\n",
    "        # (MW+1 is used if PrecursorMZ does not exist or PrecursorMZ is equals to -1/0/1)\n",
    "        #NIST use MW for filename\n",
    "        if (not filename):\n",
    "#             if ('MW: ' in row):\n",
    "#                 try:\n",
    "#                     filename = os.path.join(os.getcwd(), foldername, str(int(float(row.strip('\\n').split(\": \")[1]))) + '.csv')\n",
    "#                     precursor_mz = int(float(row.strip('\\n').split(\": \")[1]))\n",
    "#                     if (precursor_mz == -1 or precursor_mz == 0 or precursor_mz == 1):\n",
    "#                         filename = ''\n",
    "#                 except ValueError:\n",
    "#                     print('ValueError: ', row)\n",
    "#                     break\n",
    "            if ('MW: ' in row):\n",
    "                try:\n",
    "                    filename = os.path.join(os.getcwd(), foldername, str(int(float(row.strip('\\n').split(\": \")[1])) + 1) + '.csv')\n",
    "                    mw = int(float(row.strip('\\n').split(\": \")[1]))\n",
    "                except ValueError:\n",
    "                    print('ValueError: ', row)\n",
    "                    break\n",
    "                    \n",
    "#         if (not precursor_mz):\n",
    "#             if ('PrecursorMZ: ' in row):\n",
    "#                 precursor_mz = int(float(row.strip('\\n').split(\": \")[1]))\n",
    "        \n",
    "        if (not mw):\n",
    "            if ('MW: ' in row):\n",
    "                mw = int(float(row.strip('\\n').split(\": \")[1]))\n",
    "        \n",
    "        # store the current element in string and dictionary data structure\n",
    "        string += row\n",
    "        if (': ' in row):\n",
    "            try:\n",
    "                [key, val] = row.strip('\\n').split(': ', 1)\n",
    "            except ValueError:\n",
    "                print('ValueError: ')\n",
    "                print(row)\n",
    "                print(row.strip('\\n').split(': '))\n",
    "            try:\n",
    "                if (key in item_dict.keys()):\n",
    "                    item_dict[key] = [item_dict[key]]\n",
    "                    item_dict[key].append(val)\n",
    "                    repeating_key = True\n",
    "                else:\n",
    "                    item_dict[key] = val\n",
    "            except IndexError:\n",
    "                print('Index error: ', item_dict)\n",
    "        else:\n",
    "            [ms, score] = row.strip('\\n').split(' ')\n",
    "            if ('MS' in item_dict.keys()):\n",
    "                item_dict['MS'].append(ms)\n",
    "            else:\n",
    "                item_dict['MS'] = [ms]\n",
    "            if ('Score' in item_dict.keys()):\n",
    "                item_dict['Score'].append(score)\n",
    "            else:\n",
    "                item_dict['Score'] = [score]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def df_check_precursorMZ_MS(df, output_df, problematic_df):\n",
    "#     precursorMZ = df.iloc[0]['PrecursorMZ']\n",
    "#     if (abs(precursorMZ - df.iloc[0]['MS']) >= 1):\n",
    "#         output_df = output_df.append(df.iloc[0], ignore_index=True)\n",
    "#     else:\n",
    "#         problematic_df = problematic_df.append(df.iloc[0], ignore_index=True) \n",
    "#     return output_df, problematic_df\n",
    "\n",
    "\n",
    "# def s_check_precursorMZ_MS2(series, output_df):\n",
    "#     precursorMZ = series['PrecursorMZ']\n",
    "#     if (type(precursorMZ) == list):\n",
    "#         precursorMZ = precursorMZ[0]\n",
    "#     if (abs(precursorMZ - series['MS2']) >= 1):\n",
    "#         output_df = output_df.append(series, ignore_index=True)\n",
    "#     return output_df\n",
    "    \n",
    "def clean_data(df, filename):\n",
    "    count = 0\n",
    "    # filter out rows with Score value < 10\n",
    "    #NIST how to set the score value <200\n",
    "    df = df[df['Score'] >= 200]\n",
    "    \n",
    "    # output_df is the processed (cleaned) DataFrame that will be returned\n",
    "    output_df = pd.DataFrame(columns=['Name', 'InChIKey', 'ExactMass','NIST#', 'Formula', 'MW', 'MS', 'Score','RI_estimated','RI_predicted'])    \n",
    "    \n",
    "      #NIST gc-ms data ms accuracy is low, not considering precursor and ms2 \n",
    "#     # problematic_df is the DataFrame that contains items with only one row and its PrecursorMZ is similar to its MS2\n",
    "#     problematic_df = pd.DataFrame(columns=['Name', 'InChIKey', 'ExactMass', 'Precursor_type', 'PrecursorMZ', 'MW', 'MS2', 'Score'])    \n",
    "        \n",
    "#     if (df.shape[0] < 2):\n",
    "#         # check for similar PrecursorMZ and MS2\n",
    "#         output_df, problematic_df = df_check_precursorMZ_MS2(df, output_df, problematic_df)\n",
    "            \n",
    "#         return output_df, problematic_df\n",
    "    \n",
    "    df = df.sort_values(by=['InChIKey', 'MS'], ascending=True)\n",
    "    \n",
    "    df_columns = ['Name', 'ExactMass', 'MW']\n",
    "    inchikeys = df['InChIKey'].unique().tolist()\n",
    "    \n",
    "    for inchikey in inchikeys:\n",
    "        working_df = df[df['InChIKey'] == inchikey]\n",
    "        \n",
    "#         if (working_df.shape[0] < 2):\n",
    "#             # check for similar PrecursorMZ and MS2\n",
    "#             output_df, problematic_df = df_check_precursorMZ_MS2(working_df, output_df, problematic_df)\n",
    "#             continue\n",
    "        \n",
    "        output_MS = working_df.iloc[0]['MS'] # to store the chosen MS(MS with the highest score is chosen)\n",
    "        output_MS_score = working_df.iloc[0]['Score'] # to store the chosen MS score\n",
    "        output_score = working_df.iloc[0]['Score'] # to store the accumulated score\n",
    "        retain_prev = False\n",
    "        \n",
    "        for i in range(1, working_df.shape[0]):\n",
    "            # update prev_row and curr_row\n",
    "            if not (retain_prev):\n",
    "                prev_row = working_df.iloc[i-1].copy()\n",
    "            curr_row = working_df.iloc[i]\n",
    "            curr_row_MS = curr_row['MS']\n",
    "            \n",
    "            # check if both MS2 should be group together by using the formula\n",
    "            # if they should be group together, we do the following:\n",
    "            # 1. add up the score\n",
    "            # 2. update output_MS2 to the one with the higher score\n",
    "            # 3. check for all other columns other than (InChIKey, MS2, Score),\n",
    "            #    if there are different values, we append them into list\n",
    "#             if ( (abs(output_MS2 - curr_row_MS2) * 1000000 / output_MS2) < 1000 ):\n",
    "#             if ( abs(output_MS - curr_row_MS) <= 0.7 ):\n",
    "            if (abs(output_MS - curr_row_MS) <=1):\n",
    "                retain_prev = True\n",
    "                output_score += curr_row['Score']\n",
    "                if (curr_row['Score'] > output_MS_score):\n",
    "                    output_MS_score = curr_row['Score']\n",
    "                    output_MS = curr_row_MS\n",
    "                for column in df_columns:\n",
    "                    if (type(prev_row[column]) == list):\n",
    "                        if (curr_row[column] not in prev_row[column]):\n",
    "                            prev_row[column].append(curr_row[column])\n",
    "                    elif (prev_row[column] != curr_row[column]):\n",
    "                        prev_row[column] = [prev_row[column]]\n",
    "                        prev_row[column].append(curr_row[column])\n",
    "                        \n",
    "            # if they shouldn't be group together, we update the MS2 and Score\n",
    "            # to the chosen MS2 (output_MS2) and accumulated score (output_score)\n",
    "            # and append the series to output_df\n",
    "            else:\n",
    "                retain_prev = False\n",
    "                prev_row['MS'] = output_MS\n",
    "                prev_row['Score'] = output_score\n",
    "                \n",
    "                # check for similar PrecursorMZ and MS2\n",
    "                output_df = s_check_precursorMZ_MS(prev_row, output_df)\n",
    "                output_MS2 = curr_row['MS']\n",
    "                output_MS2_score = curr_row['Score']\n",
    "                output_score = curr_row['Score']\n",
    "\n",
    "        # handling the last row\n",
    "        if (retain_prev):\n",
    "            prev_row['MS'] = output_MS\n",
    "            prev_row['Score'] = output_score\n",
    "            \n",
    "#             # check for similar PrecursorMZ and MS2\n",
    "#             output_df = s_check_precursorMZ_MS(prev_row, output_df)\n",
    "#         else:\n",
    "#             # check for similar PrecursorMZ and MS2\n",
    "#             output_df = s_check_precursorMZ_MS(curr_row, output_df)\n",
    "\n",
    "    output_df.sort_values(by=['InChIKey', 'Score'], ascending=False, inplace=True)\n",
    "#     return output_df, problematic_df\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 226/1056 [01:02<07:29,  1.85it/s]E:\\anaconda\\envs\\MRMdatabase\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (6,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      " 48%|████▊     | 507/1056 [09:42<22:28,  2.46s/it]E:\\anaconda\\envs\\MRMdatabase\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "100%|██████████| 1056/1056 [17:20<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "### this code block might take around 20 minutes to run\n",
    "\n",
    "\n",
    "read_folder = os.path.join(os.getcwd(), foldername) # folder that we will be reading data from\n",
    "cleaned_folder = os.path.join(read_folder, 'cleaned_data') # folder to store cleaned data\n",
    "# problematic_folder = os.path.join(read_folder, 'problematic_data') # folder to store cleaned data\n",
    "\n",
    "if not (os.path.exists(cleaned_folder)):\n",
    "    os.makedirs(cleaned_folder)\n",
    "    \n",
    "if not (os.path.exists(problematic_folder)):\n",
    "    os.makedirs(problematic_folder)\n",
    "\n",
    "## get list of csv file names\n",
    "cwd = os.getcwd()\n",
    "os.chdir(read_folder)\n",
    "listdir = glob.glob('*.csv')\n",
    "os.chdir(cwd)\n",
    "    \n",
    "# for filename in listdir:\n",
    "for filename in tqdm(listdir):\n",
    "    output_filename = os.path.join(cleaned_folder, filename.strip('.csv') + '_cleaned.csv')\n",
    "#     problematic_output_filename = os.path.join(problematic_folder, filename.strip('.csv') + '_problematic.csv')\n",
    "    readf = os.path.join(read_folder, filename)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(readf)\n",
    "    except OSError:\n",
    "        print(readf)\n",
    "\n",
    "    # only these columns are needed for the output of the algorithm\n",
    "    df = df[['Name', 'InChIKey', 'ExactMass','NIST#', 'Formula', 'MW', 'MS', 'Score','RI_estimated','RI_predicted']]\n",
    "\n",
    "#Apr16 whether to save the problematic dataframe\n",
    "    df, problematic_df = clean_data(df, filename)\n",
    "    \n",
    "    if (problematic_df.shape[0] > 0):\n",
    "        problematic_df.to_csv(problematic_output_filename, header=True, mode='a', index=False)\n",
    "    if (df.shape[0] > 0):\n",
    "        df.to_csv(output_filename, header=True, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{57.07: 100.0}\n",
      "{41.04036: 400.0, 41.38723: 21.428571, 57.07064: 325.0, 59.15269: 21.428571}\n",
      "{43.0281: 111.21, 57.0441: 223.26}\n",
      "{57.045586: 64.188508}\n"
     ]
    }
   ],
   "source": [
    "# algorithm 4\n",
    "\n",
    "cleaned_folder = os.path.join(read_folder, 'cleaned_data3')\n",
    "test_file = os.path.join(cleaned_folder, '74_cleaned.csv')\n",
    "search_range = 0.7\n",
    "\n",
    "df = pd.read_csv(test_file)\n",
    "df.sort_values(by=['Name', 'MS2'], inplace=True)\n",
    "\n",
    "names = df['Name'].unique().tolist()\n",
    "\n",
    "def append_to_list(df):\n",
    "    ms2_dict = pd.Series(df['Score'].array, index=df['MS2']).to_dict()\n",
    "    return ms2_dict\n",
    "\n",
    "for name in names:\n",
    "    target_item_df = df[df['Name'] == name].copy()\n",
    "    interfering_items_s = df[df['Name'] != name]\n",
    "    interfering_items_s = interfering_items_s.groupby(['Name']).apply(append_to_list)\n",
    "\n",
    "    selected_interfering_items = {}\n",
    "    \n",
    "    # populate output_1_dict with target product and its corresponding score\n",
    "    output_1_dict = {}\n",
    "    for i, data in target_item_df.iterrows():\n",
    "        ms2 = data['MS2']\n",
    "        score = data['Score']\n",
    "        output_1_dict[ms2] = score\n",
    "    \n",
    "    # add up score if ms2 in interfering item is close to target product (+-0.7 search range)\n",
    "    for ms2 in output_1_dict.keys():\n",
    "#         print('-------------------')\n",
    "        for i, ms2_dict in interfering_items_s.iteritems():\n",
    "#             print('***************')\n",
    "            for key, val in ms2_dict.items():\n",
    "                diff = ms2 - key\n",
    "                if abs(diff) > search_range:\n",
    "                    if diff < 0:\n",
    "                        break\n",
    "                else:\n",
    "                    output_1_dict[ms2] += val\n",
    "                    if ms2 in selected_interfering_items.keys():\n",
    "                        selected_interfering_items[ms2].append(i)\n",
    "                    else:\n",
    "                        selected_interfering_items[ms2] = [i]\n",
    "                    \n",
    "#     print(output_1_dict)\n",
    "    output_1 = [max(output_1_dict, key=output_1_dict.get)]\n",
    "    output_1.append(output_1_dict[output_1[0]])\n",
    "#     print(output_1)\n",
    "#     print(selected_interfering_items)\n",
    "    if output_1[0] not in selected_interfering_items.keys():\n",
    "        # handler if all interfering items do not have MS2 within +-0.7 search range with output_1\n",
    "        print('no')\n",
    "    else:\n",
    "        selected_interfering_items_s = interfering_items_s[interfering_items_s.index.isin(selected_interfering_items[output_1[0]])]\n",
    "#         print(type(selected_interfering_items_s))\n",
    "        y_dict = {}\n",
    "        for name, curr in selected_interfering_items_s.iteritems():\n",
    "            print(curr)\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
